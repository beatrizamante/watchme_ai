Training Guide
==============

This guide covers training custom models for both YOLO object detection and OSNet person re-identification in the WatchMe AI Backend system.

Overview
--------

The WatchMe AI Backend supports training of two main model types:

* **YOLO Models**: For person detection and bounding box extraction
* **OSNet Models**: For person re-identification and embedding generation

Each model type requires specific dataset formats and training configurations.

YOLO Training
-------------

Dataset Preparation
~~~~~~~~~~~~~~~~~~~

**Dataset Structure:**

.. code-block:: text

   src/dataset/yolo/
   ├── images/
   │   ├── train/
   │   │   ├── image1.jpg
   │   │   ├── image2.jpg
   │   │   └── ...
   │   ├── val/
   │   │   ├── image1.jpg
   │   │   ├── image2.jpg
   │   │   └── ...
   │   └── test/
   │       ├── image1.jpg
   │       ├── image2.jpg
   │       └── ...
   └── labels/
       ├── train/
       │   ├── image1.txt
       │   ├── image2.txt
       │   └── ...
       ├── val/
       │   ├── image1.txt
       │   ├── image2.txt
       │   └── ...
       └── test/
           ├── image1.txt
           ├── image2.txt
           └── ...

**Label Format (YOLO format):**

Each `.txt` file contains one line per object:

.. code-block:: text

   class_id center_x center_y width height

Where coordinates are normalized (0-1):

.. code-block:: text

   0 0.5 0.5 0.3 0.8

Example for person detection (class 0 = person):

.. code-block:: text

   0 0.456 0.392 0.123 0.654
   0 0.789 0.123 0.087 0.456

**Configuration File (`data.yaml`):**

.. code-block:: yaml

   # filepath: src/dataset/yolo/dataset.yaml
   path: src/dataset/yolo
   train: images/train
   val: images/val
   test: images/test

   nc: 1  # number of classes
   names: ['person']  # class names

Training Configuration
~~~~~~~~~~~~~~~~~~~~~~

**Environment Variables:**

.. code-block:: bash

   # .env
   YOLO_DATASET_PATH=src/dataset/yolo
   YOLO_PROJECT_PATH=src/infrastructure/yolo/projects
   YOLO_MODEL_PATH=yolov11n.pt  # Base model
   YOLO_EPOCHS=100
   YOLO_BATCH_SIZE=16
   YOLO_IMAGE_SIZE=640
   YOLO_LEARNING_RATE=0.01

**Training Script:**

.. code-block:: python

   # python -m src.infrastructure.yolo.scripts.train

   from src.infrastructure.yolo.core.train import YOLOTrainer
   from config import YOLOSettings

   def main():
       settings = YOLOSettings()
       trainer = YOLOTrainer()

       # Train the model
       trainer.train(
           data_config=settings.YOLO_DATASET_PATH + "/data.yaml",
           epochs=settings.YOLO_EPOCHS,
           batch_size=settings.YOLO_BATCH_SIZE,
           image_size=settings.YOLO_IMAGE_SIZE,
           project=settings.YOLO_PROJECT_PATH,
           name="person_detection"
       )

   if __name__ == "__main__":
       main()

**Advanced Training Options:**

.. code-block:: python

   trainer.train(
       data_config="src/dataset/yolo/data.yaml",
       epochs=200,
       batch_size=32,
       image_size=640,
       project="src/infrastructure/yolo/projects",
       name="advanced_person_detection",

       # Advanced parameters
       patience=50,           # Early stopping patience
       save_period=10,        # Save checkpoint every N epochs
       cache=True,           # Cache images for faster training
       device='0',           # GPU device
       workers=8,            # Number of dataloader workers
       optimizer='AdamW',    # Optimizer choice
       lr0=0.01,            # Initial learning rate
       lrf=0.01,            # Final learning rate
       momentum=0.937,       # SGD momentum
       weight_decay=0.0005,  # Optimizer weight decay
       warmup_epochs=3,      # Warmup epochs
       warmup_momentum=0.8,  # Warmup initial momentum
       box=7.5,             # Box loss gain
       cls=0.5,             # Classification loss gain
       dfl=1.5,             # Distribution focal loss gain

       # Augmentation
       hsv_h=0.015,         # Image HSV-Hue augmentation
       hsv_s=0.7,           # Image HSV-Saturation augmentation
       hsv_v=0.4,           # Image HSV-Value augmentation
       degrees=0.0,         # Image rotation (+/- deg)
       translate=0.1,       # Image translation (+/- fraction)
       scale=0.5,           # Image scale (+/- gain)
       shear=0.0,           # Image shear (+/- deg)
       perspective=0.0,     # Image perspective (+/- fraction)
       flipud=0.0,          # Image flip up-down (probability)
       fliplr=0.5,          # Image flip left-right (probability)
       mosaic=1.0,          # Image mosaic (probability)
       mixup=0.0,           # Image mixup (probability)
       copy_paste=0.0       # Segment copy-paste (probability)
   )

Resume Training
~~~~~~~~~~~~~~~

.. code-block:: python

   # Resume from checkpoint
   trainer.train(
       resume=True,  # Resume from last checkpoint
       # or specify specific checkpoint:
       # resume="src/infrastructure/yolo/projects/person_detection/weights/last.pt"
   )

Hyperparameter Tuning
~~~~~~~~~~~~~~~~~~~~~

**Using Ray Tune:**

.. code-block:: python

   from src.infrastructure.yolo.core.tune import HyperparameterTuner

   def tune_hyperparameters():
       tuner = HyperparameterTuner()

       # Define search space
       search_space = {
           "lr0": (0.0001, 0.1),          # Learning rate range
           "batch_size": [16, 32, 64],     # Batch size options
           "optimizer": ["SGD", "Adam", "AdamW"],  # Optimizer options
           "weight_decay": (0.0001, 0.001),  # Weight decay range
           "momentum": (0.8, 0.95),        # Momentum range
       }

       # Run tuning
       best_config = tuner.tune(
           data_config="src/dataset/yolo/data.yaml",
           search_space=search_space,
           num_samples=20,  # Number of trials
           max_epochs=50,   # Max epochs per trial
           gpus_per_trial=1
       )

       print(f"Best configuration: {best_config}")

   if __name__ == "__main__":
       tune_hyperparameters()

**Manual Grid Search:**

.. code-block:: python

   learning_rates = [0.001, 0.01, 0.1]
   batch_sizes = [16, 32, 64]

   best_map = 0
   best_config = None

   for lr in learning_rates:
       for batch_size in batch_sizes:
           print(f"Training with lr={lr}, batch_size={batch_size}")

           results = trainer.train(
               data_config="src/dataset/yolo/data.yaml",
               epochs=50,
               lr0=lr,
               batch_size=batch_size,
               project="src/infrastructure/yolo/projects",
               name=f"tune_lr{lr}_bs{batch_size}"
           )

           # Extract mAP from results
           map_score = results.results_dict['metrics/mAP50(B)']

           if map_score > best_map:
               best_map = map_score
               best_config = {'lr': lr, 'batch_size': batch_size}

   print(f"Best config: {best_config} with mAP: {best_map}")

OSNet Training
--------------

Dataset Preparation
~~~~~~~~~~~~~~~~~~~

**Dataset Structure (Market-1501 format):**

.. code-block:: text

   src/dataset/osnet/
   ├── bounding_box_train/
   │   ├── 0001_c1s1_000151_01.jpg  # ID_camera_sequence_frame
   │   ├── 0001_c1s1_000376_03.jpg
   │   ├── 0002_c1s1_000301_01.jpg
   │   └── ...
   ├── bounding_box_test/
   │   ├── 0001_c2s1_001976_01.jpg
   │   ├── 0001_c4s1_002501_01.jpg
   │   └── ...
   ├── query/
   │   ├── 0001_c1s1_001051_00.jpg
   │   ├── 0002_c1s1_000776_00.jpg
   │   └── ...
   └── gt_bbox/  # Optional: ground truth bounding boxes
       ├── 0001_c1s1_000151_01.txt
       └── ...

**Filename Convention:**

.. code-block:: text

   {person_id}_{camera_id}s{sequence_id}_{frame_number}_{additional_id}.jpg

   Examples:
   0001_c1s1_000151_01.jpg  # Person 1, Camera 1, Sequence 1, Frame 151
   0042_c3s2_001234_02.jpg  # Person 42, Camera 3, Sequence 2, Frame 1234

**Data Split Configuration:**

.. code-block:: python

   # src/infrastructure/osnet/core/train.py

   data_config = {
       'root': 'src/dataset/osnet',
       'sources': ['market1501'],  # Dataset type
       'targets': ['market1501'],
       'height': 256,              # Image height
       'width': 128,               # Image width
       'combineall': False,        # Combine train and test for training
       'transforms': ['random_flip', 'random_crop', 'normalize'],
       'k_tfm': 1,                # Number of transform versions per image
       'load_train_targets': False
   }

Training Configuration
~~~~~~~~~~~~~~~~~~~~~~

**Environment Variables:**

.. code-block:: bash

   # .env
   OSNET_DATASET_PATH=src/dataset/osnet
   OSNET_SAVE_DIR=src/infrastructure/osnet/client
   OSNET_NUM_CLASSES=751        # Number of unique person IDs
   OSNET_EPOCHS=100
   OSNET_BATCH_SIZE=32
   OSNET_LEARNING_RATE=0.00035
   OSNET_ARCHITECTURE=osnet_x1_0

**Training Script:**

.. code-block:: python

   # python -m src.infrastructure.osnet.scripts.train

   import torchreid
   from src.infrastructure.osnet.core.train import OSNetTrainer
   from config import OSNetSettings

   def main():
       settings = OSNetSettings()
       trainer = OSNetTrainer()

       # Configure data manager
       datamanager = torchreid.data.ImageDataManager(
           root=settings.OSNET_DATASET_PATH,
           sources='market1501',
           targets='market1501',
           height=256,
           width=128,
           batch_size_train=settings.OSNET_BATCH_SIZE,
           batch_size_test=100,
           transforms=['random_flip', 'random_crop', 'normalize'],
           num_instances=4,          # Number of instances per identity in a batch
           train_sampler='RandomIdentitySampler'
       )

       # Configure model
       model = torchreid.models.build_model(
           name=settings.OSNET_ARCHITECTURE,
           num_classes=datamanager.num_train_pids,
           loss='triplet',
           pretrained=True
       )

       # Configure optimizer
       optimizer = torchreid.optim.build_optimizer(
           model,
           optim='adam',
           lr=settings.OSNET_LEARNING_RATE
       )

       # Configure scheduler
       scheduler = torchreid.optim.build_lr_scheduler(
           optimizer,
           lr_scheduler='single_step',
           stepsize=20
       )

       # Configure engine
       engine = torchreid.engine.ImageTripletEngine(
           datamanager,
           model,
           optimizer=optimizer,
           scheduler=scheduler,
           label_smooth=True,
       )

       # Start training
       engine.run(
           save_dir=settings.OSNET_SAVE_DIR,
           max_epoch=settings.OSNET_EPOCHS,
           eval_freq=10,
           print_freq=20,
           test_only=False,
           resume='',  # Path to checkpoint to resume from
       )

   if __name__ == "__main__":
       main()

**Advanced Training Configuration:**

.. code-block:: python

   # Advanced OSNet training with custom settings

   def advanced_training():
       # Data augmentation configuration
       transform_configs = [
           'random_flip',
           'random_crop',
           'random_erase',
           'color_jitter',
           'random_gray_scale',
           'normalize'
       ]

       # Multiple dataset sources
       datamanager = torchreid.data.ImageDataManager(
           root='src/dataset/osnet',
           sources=['market1501', 'dukemtmcreid'],  # Multiple datasets
           targets=['market1501', 'dukemtmcreid'],
           height=256,
           width=128,
           batch_size_train=64,
           batch_size_test=100,
           transforms=transform_configs,
           num_instances=4,
           num_cams=8,              # Number of cameras per identity
           num_datasets=2,          # Number of datasets
           combineall=True,         # Combine all data for training
           load_train_targets=True,
           k_tfm=1,
           train_sampler='RandomIdentitySampler'
       )

       # Model with advanced configurations
       model = torchreid.models.build_model(
           name='osnet_ibn_x1_0',   # OSNet with Instance-Batch Normalization
           num_classes=datamanager.num_train_pids,
           loss=['triplet', 'softmax'],  # Combined losses
           pretrained=True,
           use_gpu=True
       )

       # Advanced optimizer configuration
       optimizer = torchreid.optim.build_optimizer(
           model,
           optim='adamw',           # AdamW optimizer
           lr=0.00035,
           weight_decay=5e-4,
           eps=1e-8,
           amsgrad=True
       )

       # Learning rate scheduler
       scheduler = torchreid.optim.build_lr_scheduler(
           optimizer,
           lr_scheduler='cosine',   # Cosine annealing
           max_epoch=120,
           restart_epoch=40,
           eta_min=1e-7
       )

       # Training engine with advanced settings
       engine = torchreid.engine.ImageTripletEngine(
           datamanager,
           model,
           optimizer=optimizer,
           scheduler=scheduler,
           use_gpu=True,
           label_smooth=True,
           margin=0.3,              # Triplet loss margin
           weight_t=1.0,            # Triplet loss weight
           weight_x=1.0,            # Softmax loss weight
       )

       # Training execution
       engine.run(
           save_dir='src/infrastructure/osnet/client',
           max_epoch=120,
           eval_freq=5,             # Evaluate every 5 epochs
           print_freq=50,           # Print every 50 iterations
           test_only=False,
           visrank=True,            # Save ranking visualization
           visrank_topk=10,         # Top-k for visualization
           use_metric_cuhk03=False,
           ranks=[1, 5, 10, 20],    # Rank metrics to compute
           rerank=False,            # Post-processing re-ranking
           save_checkpoint=True,    # Save regular checkpoints
           resume='',               # Resume from checkpoint
       )

Transfer Learning
~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Fine-tune from pre-trained checkpoint

   def transfer_learning():
       # Load pre-trained model
       model = torchreid.models.build_model(
           name='osnet_x1_0',
           num_classes=751,  # Original Market-1501 classes
           pretrained=False
       )

       # Load pre-trained weights
       checkpoint = torch.load('pretrained_osnet.pth.tar')
       model.load_state_dict(checkpoint['state_dict'])

       # Modify classifier for new dataset
       num_new_classes = 500  # Your dataset's number of identities
       model.classifier = nn.Linear(model.feature_dim, num_new_classes)

       # Freeze backbone (optional)
       for name, param in model.named_parameters():
           if 'classifier' not in name:
               param.requires_grad = False

       # Continue with training setup...

Monitoring Training Progress
----------------------------

TensorBoard Integration
~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   # Monitor training with TensorBoard

   # Start TensorBoard
   # tensorboard --logdir=src/infrastructure/yolo/projects/runs

   # For OSNet, logs are automatically saved to the save_dir
   # tensorboard --logdir=src/infrastructure/osnet/client

Weights & Biases Integration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

   import wandb

   # Initialize W&B
   wandb.init(
       project="watchme-training",
       config={
           "architecture": "osnet_x1_0",
           "dataset": "market1501",
           "epochs": 100,
           "batch_size": 32,
           "learning_rate": 0.00035
       }
   )

   # Log metrics during training
   wandb.log({
       "epoch": epoch,
       "loss": loss_value,
       "rank1": rank1_accuracy,
       "mAP": mean_average_precision
   })

Custom Logging
~~~~~~~~~~~~~~

.. code-block:: python

   import logging
   from datetime import datetime

   # Setup custom logging
   logging.basicConfig(
       level=logging.INFO,
       format='%(asctime)s - %(levelname)s - %(message)s',
       handlers=[
           logging.FileHandler(f'training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
           logging.StreamHandler()
       ]
   )

   logger = logging.getLogger(__name__)

   # Log training progress
   logger.info(f"Epoch {epoch}/{max_epochs} - Loss: {loss:.4f} - Rank-1: {rank1:.2f}%")

Model Evaluation
----------------

YOLO Evaluation
~~~~~~~~~~~~~~~

.. code-block:: python

   from src.infrastructure.yolo.core.evaluate import YOLOEvaluator

   def evaluate_yolo():
       evaluator = YOLOEvaluator()

       # Evaluate on test set
       results = evaluator.evaluate(
           model_path="src/infrastructure/yolo/projects/person_detection/weights/best.pt",
           data_config="src/dataset/yolo/data.yaml",
           split="test"
       )

       print(f"mAP@0.5: {results['metrics/mAP50(B)']:.4f}")
       print(f"mAP@0.5:0.95: {results['metrics/mAP50-95(B)']:.4f}")
       print(f"Precision: {results['metrics/precision(B)']:.4f}")
       print(f"Recall: {results['metrics/recall(B)']:.4f}")

OSNet Evaluation
~~~~~~~~~~~~~~~~

.. code-block:: python

   def evaluate_osnet():
       # Evaluation is automatically performed during training
       # Results are saved in the save_dir

       # Manual evaluation
       model = torchreid.models.build_model(
           name='osnet_x1_0',
           num_classes=751,
           pretrained=False
       )

       # Load trained weights
       checkpoint = torch.load('src/infrastructure/osnet/client/model.pth.tar')
       model.load_state_dict(checkpoint['state_dict'])

       # Evaluate
       engine = torchreid.engine.ImageTripletEngine(
           datamanager, model, optimizer, scheduler
       )

       engine.run(
           save_dir='src/infrastructure/osnet/client',
           max_epoch=0,  # No training, just evaluation
           test_only=True,
           visrank=True,
           ranks=[1, 5, 10, 20]
       )

Best Practices
--------------

Data Quality
~~~~~~~~~~~~

1. **Ensure high-quality annotations** for YOLO training
2. **Maintain consistent identity labeling** for OSNet
3. **Balance dataset classes** to prevent bias
4. **Use data augmentation** to improve robustness

Training Tips
~~~~~~~~~~~~~

1. **Start with pre-trained models** for faster convergence
2. **Use learning rate scheduling** for better optimization
3. **Monitor validation metrics** to prevent overfitting
4. **Save regular checkpoints** to recover from failures

Hardware Optimization
~~~~~~~~~~~~~~~~~~~~~

1. **Use mixed precision training** for faster training on modern GPUs
2. **Increase batch size** with available GPU memory
3. **Use multiple GPUs** for distributed training when available
4. **Monitor GPU utilization** to ensure efficient resource usage

.. code-block:: python

   # Mixed precision training example
   from torch.cuda.amp import GradScaler, autocast

   scaler = GradScaler()

   for data in dataloader:
       optimizer.zero_grad()

       with autocast():
           outputs = model(data)
           loss = criterion(outputs, targets)

       scaler.scale(loss).backward()
       scaler.step(optimizer)
       scaler.update()

This comprehensive training guide provides everything needed to train custom models for the WatchMe AI Backend system, from basic setups to advanced configurations and optimization techniques.
